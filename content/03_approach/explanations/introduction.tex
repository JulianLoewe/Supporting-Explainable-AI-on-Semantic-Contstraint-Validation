In this section, the focus is on using the validation results available as model-validation-result function $\Theta$ to get insights into the model behavior (see lemma \ref{S:problem_explaining_ai_model_with_constraints}) and explain predictions made by the model, when possible. $\Theta$ was generated by validating the model $M_\theta$ and the samples extracted from a knowledge graph $G$ with user-defined prediction and data constraints. Because of the different semantics as explained in section \ref{section_further_types_of_constraints} different usage scenarios arise:

\begin{enumerate}
    \item Training a model, which conforms to specific requirements
        \begin{enumerate}
            \item Understand the influence of bad data (w.r.t. data constraints) used to train the model 
            \item Analyze why the model may have made bad decisions w.r.t. prediction constraints
        \end{enumerate}
    \item During the model inference explain a model prediction made based on a problem instance w.r.t. prediction constraints
\end{enumerate}

These scenarios are relevant for different kind of users. The machine learning engineer is interested in creating models, which conform to requirements of the client and have as less errors as possible. In that case, the explanations can be used to gain insights into the model behavior (scenario 1), which then might be used to improve the model. For example, through changing the hyperparameters of the model, being able to perform further data cleansing or further feature engineering. The next group of people are the ones offering the model to the world. For them it is important to have a model, which makes predictions conforming to governmental regulations, or scientific facts (e.g., medical or physical) and social principles. As they are not interested in improving the model directly, but instead want to see that the model they are selling is correct (scenarios 1b and 2). Finally, there are the users of the model. As a user, one wants to be able to justify the decision made by the model and one might have certain constraints on the model, which should be respected and can be used to explain the predictions made by the model (scenario 2). In this thesis, a graphical approach is used for explanations, as this is meant to facilitate the understanding of the model by the different kinds of user groups and can be interpreted properly according to the given scenario.

The following sections build a framework for visualizing constraint validation results, while also applying the framework to confusion matrices and to decision trees.