\documentclass[../../../thesis]{subfiles}
\begin{document}
    AI is ``the study and design of intelligent agents, [which] [...] take actions that maximize [their] chances of success'' \cite{russell2002artificial}. When this definition of AI is extended to explainable AI, the goal is to produce agents whose behavior can be understood by humans. ``[It allows] users to comprehend and trust the results and outputs created by machine learning algorithms'' \cite{ibmExplainableAI}. ``[That is] explainability [is the capability of the model to] summarize the reasons [for their] behavior, gain the trust of the users, or produce insights about the causes of their decisions'' \cite{gilpin2018explaining}. This is in contrast to the interpretability of AI, which only requires to ``[...] describe the internals of a
    system in a way that is understandable to humans'' \cite{gilpin2018explaining}. To tackle explainable AI in the context of semantic constraint validation, first, the most needed machine learning concepts are introduced, followed by a more specific introduction to the covered models.
\end{document}