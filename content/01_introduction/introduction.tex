AI is called explainable when a user of an AI model can understand the predictions a model makes. The user is enabled to not only interpret the model but trust the model through given reasoning of why a decision has been made. \cite{ibmExplainableAI, gilpin2018explaining} 

Knowledge graphs are used to store and connect data, promoting the cooperation between machines and humans. Therefore the data and its entities are stored in a way understandable for both. \cite{berners2002new}

Data is extracted from knowledge graphs for statistical analysis \cite{moghaddam2021literal2feature,cheng2011automated,narasimha2011liddm}. However, the process only uses the context of the extracted entities during the data mining process. On the other hand, embeddings represent the entity through its properties and links to other entities (i.e., latent vectors). Therefore, they incorporate the semantic context for later usage (e.g., clustering by embedding-based similarity measures \cite{embedding_similarity_measure}) but lake for explainability.
Finally, there are rule-based systems. Mohamed et al. \cite{excut} mine rules for comprehensible cluster labels. Halliwell et al. \cite{Halliwell2021} generate ground truth explanations for the link prediction task based on handcrafted rules. Even handcrafted rules show to suffer from understandability in some cases \cite{Halliwell2021}. 

In the area of the semantic web, SHACL engines are known for their capability to validate knowledge graphs against a set of constraints (i.e., rules). Therefore rating entities based on their integrity and trustability. 

This work aims to exploit SHACL engines to support the explainability of predictions made by machine learning models. The approach is naturally based on handcrafted constraints (i.e., rules), which should facilitate the generation of understandable explanations. Based on the constraints the approach will additionally increase the interpretability of AI models by annotating the model with constraint validation results. The constraint validation results will be based on the semantic context provided by a knowledge graph. 
Standard interpretability methods (e.g., LIME \cite{ribeiro2016should}, or Partial Dependence Plots \cite{friedman2001greedy}) do not take into consideration the context of the entities used to train the model or make predictions. Therefore Using the semantic context of entities in the described way is novel and closes a hole in interpretability methods. 

Recently, machine-learning-based techniques have been used to accelerate COVID-19-related drug discovery during the pandemic \cite{zhang2021ai}. Drugs found through these kinds of techniques need to be validated carefully. On a large scale, this highlights the importance of explainable AI. Most likely, the lack of explainability of the AI model used for drug discovery leads to the need to validate the results by hand. However, given the expertise in validating drugs, there are for sure constraints involving the training data and the assigned targets. These could have been used for automatic validation and a step toward better explainable AI. 
    